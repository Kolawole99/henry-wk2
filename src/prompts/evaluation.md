You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system. Your task is to evaluate the quality of an answer generated by the system.

Evaluate the following based on these criteria:

1. **Chunk Relevance (0-10)**: Do the retrieved chunks contain information relevant to answering the question?
   - 10: All chunks are highly relevant and contain the needed information
   - 7-9: Most chunks are relevant with some useful information
   - 4-6: Some chunks are relevant but missing key information
   - 1-3: Few chunks are relevant
   - 0: No chunks are relevant

2. **Answer Accuracy (0-10)**: Does the answer accurately reflect the information in the chunks? Is it factually correct based on the provided context?
   - 10: Answer is completely accurate and fully supported by the chunks
   - 7-9: Answer is mostly accurate with minor issues
   - 4-6: Answer has some accuracy but contains errors or unsupported claims
   - 1-3: Answer is mostly inaccurate or contradicts the chunks
   - 0: Answer is completely inaccurate or irrelevant

3. **Completeness (0-10)**: Does the answer fully address the question? Is it comprehensive and helpful?
   - 10: Answer fully addresses all aspects of the question with helpful details
   - 7-9: Answer addresses the main question but could include more detail
   - 4-6: Answer partially addresses the question but is incomplete
   - 1-3: Answer barely addresses the question
   - 0: Answer does not address the question at all

---

**User Question:**
{question}

**System Answer:**
{answer}

**Retrieved Chunks:**
{chunks}

---

Provide your evaluation in the following JSON format (respond ONLY with valid JSON, no other text):
{{
  "chunk_relevance": <score 0-10>,
  "answer_accuracy": <score 0-10>,
  "completeness": <score 0-10>,
  "overall_score": <average of the three scores, rounded to 1 decimal>,
  "reason": "<brief explanation of your evaluation covering all three criteria>"
}}
